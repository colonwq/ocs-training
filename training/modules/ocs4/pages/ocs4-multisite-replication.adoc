= OpenShift Multisite Disaster Recovery  
:icons: font
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:
:source-highlighter: pygments
:hide-uri-scheme:

== Overview

The indent of this guide is to detail the steps and commands necessary to be able to failover an application from one `OpenShift Container Platform` (OCP) cluster to another and then failback the same application to the primary cluster. 

The necessary components are two OCP 4.6 (or greater) clusters, connectivity between the private networks, and `OpenShift Container Storage` (OCS) installed on both OCP clusters. OCS version *4.7* (or greater) is required for the `RBD Mirroring` feature to provide asynchronous storage replication. In order to also mirror or replicate the OCP metadata (pods, services, routes, etc.) from one cluster to another, this guide will make use of the Velero `Backup` and `Restore` APIs exposed via the OCP community operator `OpenShift APIs for Data Protection` or `OADP`.

=== OpenShift Multisite Connectivity

For this solution to work the OpenShift SDNs (Software Defined Networks) must be connected so that `OpenShift Container Storage` (OCS) resources can communicate, in particular the OCS Ceph Monitors on the two different clusters must have connectivity. There are various ways to connect the private SDNs and the choice for this guide is to use `Submariner`. 

NOTE: Whatever way the OCP clusters are connected, the default `IP CIDRs` (Classless inter-domain routing) for cluster and service networking must be modified on one of the clusters to avoid addressing conflicts.

`Submariner` consists of several main components that work in conjunction to securely connect workloads across multiple Kubernetes or OCP clusters, both on-premises and on public clouds:

* Gateway Engine: manages the secure tunnels to other clusters.
* Route Agent: routes cross-cluster traffic from nodes to the active Gateway Engine.
* Broker: facilitates the exchange of metadata between Gateway Engines enabling them to discover one another.
* Service Discovery: provides DNS discovery of Services across clusters.

`Submariner` does support connecting both OCP clusters installed on AWS. There is also support for connecting non-AWS OCP clusters such as those installed on VMware or Bare Metal (BM). Hybrid connectivity of one OCP cluster on AWS and the 2nd cluster on non-AWS (i.e. VMware) is also possible.

For more information on `Submariner` and the different configuration options go to (https://submariner.io/getting-started/).

==== Submariner Prerequisites

There are a new prerequisties to deploy `Submariner`. First is to modify the *install-config.yaml* configuration used with *openshift-install* before installing OpenShift so that the IP ranges for the cluster and service networks will be different as shown in examples below. 

Example for `site1` *install-config.yaml*:

[source,yaml]
----
[...]
metadata:
  name: site1
networking:
  clusterNetwork:
  - cidr: 10.6.0.0/16 <1>
    hostPrefix: 23
  machineCIDR: 10.0.0.0/16
  networkType: OpenShiftSDN
  serviceNetwork:
  - 10.16.0.0/16 <2>
[...]
----  

<1> clusterNetwork CIDR for site1
<2> serviceNetwork CIDR for site1

Example for `site2` *install-config.yaml*:

[source,yaml]
----
[...]
metadata:
  name: site2
networking:
  clusterNetwork:
  - cidr: 10.12.0.0/16 <1>
    hostPrefix: 23
  machineCIDR: 10.0.0.0/16
  networkType: OpenShiftSDN
  serviceNetwork:
  - 10.112.0.0/16 <2>
[...]
----  

<1> clusterNetwork CIDR for site2
<2> serviceNetwork CIDR for site2

For OCP on AWS the following packages needs to be available on your deploy host to install `Submariner`:

* AWS CLI
* Terraform version 0.12 (maximum version is 0.12.12)
* wget

You also need to download the `prep_for_subm.sh` script to install `Submariner` when one or both of your OCP clusters are on AWS.

[source,role="execute"]
----
curl https://raw.githubusercontent.com/submariner-io/submariner/devel/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O
----

Make `prep_for_subm.sh` executable.

[source,role="execute"]
----
chmod a+x ./prep_for_subm.sh
----

Refer to these links for additional information about prerequisites when at least one OCP instance is installed on AWS.

* https://submariner.io/getting-started/quickstart/openshift/aws/
* https://submariner.io/getting-started/quickstart/openshift/vsphere-aws/

For non-AWS OCP clusters the only requirement is to download `subctl` client that will be used for most `Submariner` commands.

[source,role="execute"]
----
curl -Ls https://get.submariner.io | bash
----

And export the path to `subctl`:

[source,role="execute"]
----
export PATH=$PATH:~/.local/bin
----

NOTE: Add exported path for `subctl` in your deploy host `.profile` or `.bashrc`.

Once the two OCP clusters are created note the location of their unique `kubeconfig` (i.e.~/site1/auth/kubeconfig). 

==== Submariner Installation

The `Submariner` installation detailed in this guide is for two non-AWS OCP clusters installed on VMware. 

NOTE: Make sure to delete any prior `broker-info.subm` file before creating a new `broker-info.subm`.

NOTE: All `subctl` commands can be executed logged into `site1` OCP cluster.

Start by deploying the `broker`.

[source,role="execute"]
----
subctl deploy-broker --kubeconfig site1/auth/kubeconfig
----
.Example output:
----
 ✓ Deploying broker 
 ✓ Creating broker-info.subm file 
 ✓ A new IPsec PSK will be generated for broker-info.subm
----
 
Now we want to create the connection between the two OCP clusters. The `gateway` *Pod* will be created on the node selected from the displayed list of available nodes. 
 
NOTE: The `--disable-nat` flag is used when the two OCP clusters are on the same network. Reference `Submariner` documentation for how to join OCP clusters using `NAT` (Network Address Translation).
 
[source,role="execute"]
----
subctl join --kubeconfig site1/auth/kubeconfig --clusterid site1 broker-info.subm --disable-nat
----
.Example output:
----
* broker-info.subm says broker is at: https://api.site1.chris.ocs.ninja:6443
? Which node should be used as the gateway? site1-fqldq-worker-975qq
⢄⡱ Discovering network details     Discovered network details:
        Network plugin:  OpenShiftSDN
        Service CIDRs:   [10.16.0.0/16]
        Cluster CIDRs:   [10.6.0.0/16]
 ✓ Discovering network details
 ✓ Validating Globalnet configurations
 ✓ Discovering multi cluster details
 ✓ Deploying the Submariner operator 
 ✓ Created operator CRDs
 ✓ Created operator namespace: submariner-operator
 ✓ Created operator service account and role
 ✓ Updated the privileged SCC
 ✓ Created lighthouse service account and role
 ✓ Updated the privileged SCC
 ✓ Created Lighthouse service accounts and roles
 ✓ Deployed the operator successfully
 ✓ Creating SA for cluster 
 ✓ Deploying Submariner 
 ✓ Submariner is up and running
----

Now do a similar command for `site2`. The displayed list of available nodes for the `gateway` *Pod* will be those for `site2` OCP instance.

[source,role="execute"]
----
subctl join --kubeconfig site2/auth/kubeconfig --clusterid site2 broker-info.subm --disable-nat
----
.Example output:
----
* broker-info.subm says broker is at: https://api.site1.chris.ocs.ninja:6443
? Which node should be used as the gateway? site2-lc8kr-worker-8j2qk
⢄⡱ Discovering network details     Discovered network details:
        Network plugin:  OpenShiftSDN
        Service CIDRs:   [10.112.0.0/16]
        Cluster CIDRs:   [10.12.0.0/16]
 ✓ Discovering network details
 ✓ Validating Globalnet configurations
 ✓ Discovering multi cluster details
 ✓ Deploying the Submariner operator 
 ✓ Created operator CRDs
 ✓ Created operator namespace: submariner-operator
 ✓ Created operator service account and role
 ✓ Updated the privileged SCC
 ✓ Created lighthouse service account and role
 ✓ Updated the privileged SCC
 ✓ Created Lighthouse service accounts and roles
 ✓ Deployed the operator successfully
 ✓ Creating SA for cluster 
 ✓ Deploying Submariner 
 ✓ Submariner is up and running
----

On the `site1` OCP that you are logged into you can validate that the `Submariner` *Pods* are running. The same *Pods* show be `Running` in `site2` in the `submariner-operator` project.

[source,role="execute"]
----
oc get pods -n submariner-operator
----
.Example output:
----
NAME                                            READY   STATUS    RESTARTS   AGE
submariner-gateway-kthdc                        1/1     Running   0          28m
submariner-lighthouse-agent-6c5755764-hjhsm     1/1     Running   0          27m
submariner-lighthouse-coredns-c4f7b6b8c-7nqxz   1/1     Running   0          27m
submariner-lighthouse-coredns-c4f7b6b8c-nt2rl   1/1     Running   0          27m
submariner-operator-6df7c9d659-9d9pm            1/1     Running   0          28m
submariner-routeagent-b476m                     1/1     Running   0          27m
submariner-routeagent-bchnj                     1/1     Running   0          27m
submariner-routeagent-glmlj                     1/1     Running   0          27m
submariner-routeagent-qgdps                     1/1     Running   0          27m
submariner-routeagent-sl2tr                     1/1     Running   0          27m
submariner-routeagent-smmdt                     1/1     Running   0          27m
----

The last step is to validate the connection between the two OCP clusters using a `subctl verify` command.

[source,role="execute"]
----
subctl verify site1/auth/kubeconfig site2/auth/kubeconfig --only connectivity --verbose
----
.Example output:
----
Performing the following verifications: connectivity
Running Suite: Submariner E2E suite
===================================
Random Seed: 1614875124
Will run 17 of 34 specs
[...]
------------------------------

Ran 11 of 34 Specs in 159.666 seconds
SUCCESS! -- 11 Passed | 0 Failed | 0 Pending | 23 Skipped
----

You can also verify the connectivity this way using `site` kubeconfig and `subctl show connections`.

[source,role="execute"]
----
subctl show connections --kubeconfig site1/auth/kubeconfig | grep connected
----
.Example output:
----
site2-lc8kr-worker-8j2qk        site2                   10.70.56.173    libreswan           10.112.0.0/16, 10.12.0.0/16             connected 
----

And then using `site2` kubeconfig.

[source,role="execute"]
----
subctl show connections --kubeconfig site2/auth/kubeconfig | grep connected
----
.Example output:
----
site1-fqldq-worker-975qq        site1                   10.70.56.191    libreswan           10.16.0.0/16, 10.6.0.0/16               connected
----

Now that the two OCP instances have their `clusterNetwork` and `serviceNetwork` connected the next step is to install `OpenShift Container Storage` version *4.7* and configure storage replication or `RDB Mirroring`.

=== Configuring Multisite Storage Replication

==== OpenShift Container Storage Installation

In order to configure storage replication between the two OCP instances `OpenShift Container Storage` must be installed first. Documentation for the deployment can be found at https://access.redhat.com/documentation/en-us/red_hat_openshift_container_storage/4.7. 

OCS deployment guides and instructions are specific to your infrastructure (i.e. AWS, VMware, BM, Azure, etc.). Install OCS version *4.7* or greater on both OCP clusters.

You can validate the successful deployment of the OCS on each OCP instance with the following:

[source,role="execute"]
----
oc get storagecluster -n openshift-storage ocs-storagecluster -o jsonpath='{.status.phase}{"\n"}'
----

If it says `Ready` on both clusters you can continue.

==== Configuring RBD Mirroring between OCS clusters

The next step will be to create the mirroring relationship between the two OCS clusters so the RBD volumes or images created using the Ceph RBD storageclass can be replicated from one OCP cluster to the other OCP cluster. For the rest of the instructions we will refer to the second cluster as the `peer` cluster.

===== Enable OMAP Generator

Omap generator is a sidecar container that when deployed with the CSI provisioner pod, generates the internal CSI omaps between the PV and the RBD image. The name of the new container is `csi-omap-generator`. This is required as static PVs are transferred across peer clusters in the DR use case, and hence is needed to preserve PVC to storage mappings.

Execute the steps on each of the OCP clusters to enable the OMAP generator.

Edit the rook-ceph-operator-config configmap and add `CSI_ENABLE_OMAP_GENERATOR` set to true.

[source,role="execute"]
----
oc patch cm rook-ceph-operator-config -n openshift-storage --type json --patch  '[{ "op": "add", "path": "/data/CSI_ENABLE_OMAP_GENERATOR", "value": "true" }]'
----
.Example output:
----
configmap/rook-ceph-operator-config patched
----

Validate that there are now 7 sidecar containers and that the `csi-omap-generator` container is now running.

[source,role="execute"]
----
oc get pods -l app=csi-rbdplugin-provisioner -o jsonpath={.items[*].spec.containers[*].name}
----
.Example output:
----
csi-provisioner csi-resizer csi-attacher csi-snapshotter csi-omap-generator csi-rbdplugin liveness-prometheus csi-provisioner csi-resizer csi-attacher csi-snapshotter csi-omap-generator csi-rbdplugin liveness-prometheus
----

NOTE: There are two `csi-rbdplugin-provisioner` pods for availability so there should be two groups of the same 7 containers for each pod.

NOTE: Repeat these steps for the peer cluster before proceeding and also repeat the validation for new `csi-omap-generator` container.

===== Create Ceph Pools for replication

In this section we create specific ceph pools that are RBD mirroring enabled. Execute the steps on each of the OCP clusters to enable mirroring.

Sample Ceph block pool that has mirroring enabled:

[source,yaml]
----
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
   name: replicapool
   namespace: openshift-storage
spec:
   replicated:
     size: 3
   mirroring:
     enabled: true
     mode: image
       # specify the schedules on which snapshots should be taken
     snapshotSchedules:
       - interval: 1h
         startTime: 00:00:00-05:00
   statusCheck:
     mirror:
       disabled: false
       interval: 60
----

Now create this new Ceph blockpool.

[source,role="execute"]
----
curl -s https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/training/modules/ocs4/attachments/replicapool.yaml | oc apply -f -
----
.Example output:
----
cephblockpool.ceph.rook.io/replicapool created
----

NOTE: Repeat the steps on the peer cluster.

===== Bootstrap Peer Clusters

In order for the `rbd-mirror` daemon to discover its peer cluster, the peer must be registered and a user account must be created. The following steps enables `Bootstrapping` peers to discover and authenticate to each other.

For `Bootstrapping` a peer cluster its bootstrap secret is required. 

NOTE: Execute the following commands on the *remote cluster* or site2 cluster.

To determine the name of the secret that contains the bootstrap secret do the following:

[source,role="execute"]
----
oc get cephblockpool.ceph.rook.io/replicapool -n openshift-storage -ojsonpath='{.status.info.rbdMirrorBootstrapPeerSecretName}{"\n"}'
----
.Example output:
----
pool-peer-token-replicapool
----

The secret `pool-peer-token-replicapool` contains all the information related to the token and needs to be injected to the peer, to fetch the decoded secret do the following:

[source,role="execute"]
----
oc get secrets pool-peer-token-replicapool -n openshift-storage -o jsonpath='{.data.token}' | base64 -d
----
.Example output:
----
eyJmc2lkIjoiNTliODkwMjEtM2VlMi00YTI1LWIwODctYjQzZWU4MGIzZGRlIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFCd3pVSmdGWVFySXhBQU5NR3AyK3BrR2hJa1dzMXBFclN4ZUE9PSIsIm1vbl9ob3N0IjoiW3YyOjEwLjExMi4yNDQuMTQ2OjMzMDAsdjE6MTAuMTEyLjI0NC4xNDY6Njc4OV0sW3YyOjEwLjExMi4xMDMuNTU6MzMwMCx2MToxMC4xMTIuMTAzLjU1OjY3ODldLFt2MjoxMC4xMTIuMTI0LjExOTozMzAwLHYxOjEwLjExMi4xMjQuMTE5OjY3ODldIn0=
----

Now get the site name from the *remote cluster* or site2 cluster:

[source,role="execute"]
----
oc get cephblockpools.ceph.rook.io replicapool -n openshift-storage -o jsonpath='{.status.mirroringInfo.summary.summary.site_name}{"\n"}'
----
.Example output:
----
59b89021-3ee2-4a25-b087-b43ee80b3dde-openshift-storage
----

NOTE: Execute the following commands on the *primary cluster* or site1 cluster.

With this decoded value, create a secret on the *primary cluster*, using the site name of the *remote cluster* as the name from prior step.

IMPORTANT: *Make sure to replace site name and secret with the values from your clusters.*

----
oc -n openshift-storage create secret generic 59b89021-3ee2-4a25-b087-b43ee80b3dde-openshift-storage --from-literal=token=eyJmc2lkIjoiNTliODkwMjEtM2VlMi00YTI1LWIwODctYjQzZWU4MGIzZGRlIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFCd3pVSmdGWVFySXhBQU5NR3AyK3BrR2hJa1dzMXBFclN4ZUE9PSIsIm1vbl9ob3N0IjoiW3YyOjEwLjExMi4yNDQuMTQ2OjMzMDAsdjE6MTAuMTEyLjI0NC4xNDY6Njc4OV0sW3YyOjEwLjExMi4xMDMuNTU6MzMwMCx2MToxMC4xMTIuMTAzLjU1OjY3ODldLFt2MjoxMC4xMTIuMTI0LjExOTozMzAwLHYxOjEwLjExMi4xMjQuMTE5OjY3ODldIn0= --from-literal=pool=replicapool
----
.Example output:
----
secret/59b89021-3ee2-4a25-b087-b43ee80b3dde-openshift-storage created
----

This completes the bootstrap process for the *primary cluster* to the *remote cluster* or site1 to site2. 

NOTE: Repeat the process switching the steps for the *remote cluster* and the *primary cluster*.

After generating the *primary cluster* secret and site name the following example command is done on the *remote cluster*.

IMPORTANT: *Make sure to replace site name and secret with the values from your clusters.*

----
oc -n openshift-storage create secret generic dc12a67b-d82c-4b7c-b3d7-60a44d973772-openshift-storage --from-literal=token=eyJmc2lkIjoiZGMxMmE2N2ItZDgyYy00YjdjLWIzZDctNjBhNDRkOTczNzcyIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFBY3pVSmdUTjZKT0JBQWlXZDNBV3UxeE41N1NrMVd0L3owaUE9PSIsIm1vbl9ob3N0IjoiW3YyOjEwLjE2LjIzOS40OjMzMDAsdjE6MTAuMTYuMjM5LjQ6Njc4OV0sW3YyOjEwLjE2LjE2My4xMzI6MzMwMCx2MToxMC4xNi4xNjMuMTMyOjY3ODldLFt2MjoxMC4xNi40NC44NTozMzAwLHYxOjEwLjE2LjQ0Ljg1OjY3ODldIn0= --from-literal=pool=replicapool
----
.Example output:
----
secret/dc12a67b-d82c-4b7c-b3d7-60a44d973772-openshift-storage created
----

This completes the bootstrap process for the *remote cluster* to the *primary cluster* or site2 to site1.

===== Create RBD Mirror Custom Resource

Replication is handled by the *rbd-mirror* daemon. The *rbd-mirror* daemon is responsible for pulling image updates from the *remote cluster*, and applying them to images within the local cluster. 

The `rbd-mirror` daemon(s) can be created using a custom resource definitions (CRD). There must be a `rbd-mirror` daemon or *Pod* created on the *primary cluster* and the *remote cluster*.

[source,yaml]
----
apiVersion: ceph.rook.io/v1
kind: CephRBDMirror
metadata:
  name: rbd-mirror
  namespace: openshift-storage
spec:
  # the number of rbd-mirror daemons to deploy
  count: 1
  peers:
    secretNames:
      # list of Kubernetes Secrets containing the peer token
      - "secret"
----

To get `secret` for the *primary cluster* or site1 to the following:

[source,role="execute"]
----
SECRET=$(oc get secrets | grep openshift-storage | awk {'print $1}')
echo $SECRET
----
.Example output:
----
59b89021-3ee2-4a25-b087-b43ee80b3dde-openshift-storage
----

Now create the `rbd-mirror` *Pod* for the *primary site*:

[source,role="execute"]
----
curl -s https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/training/modules/ocs4/attachments/rbd-mirror.yaml | sed -e "s/SECRET/${SECRET}/g" | oc apply -f -
----
.Example output:
----
cephrbdmirror.ceph.rook.io/rbd-mirror created
----

Check to see if the new`rbd-mirror` *Pod* is created and `Running`.

[source,role="execute"]
----
oc get pods -n openshift-storage | grep rbd-mirror
----
.Example output:
----
rook-ceph-rbd-mirror-a-57ccc68d88-lts87                           2/2     Running     0          5m
----

Check the status of the status of the `rbd-mirror` daemon health.

[source,role="execute"]
----
oc get cephblockpools.ceph.rook.io replicapool -n openshift-storage -o jsonpath='{.status.mirroringStatus.summary.summary}{"\n"}'
----
.Example output:
----
{"daemon_health":"OK","health":"OK","image_health":"OK","states":{}}
----

Now repeat process for *remote cluster* or site2.

IMPORTANT: Make sure to do all steps above on the *remote cluster*. The results for `SECRET` should be different as a way to check you are on the *remote cluster*.

You have now completed the steps for configuring *RBD Mirroring* between the *primary cluster* and the *remote cluster* or site1 and site2. The next sections will cover how to enable RBD images or volumes for mirroring data from site1 to site2 asynchronously. Also, using a sample application, detailed instructions will be provided on how to `failover` from site1 to site2 and how to `failback` the application all the while preserving the persistent data. 


