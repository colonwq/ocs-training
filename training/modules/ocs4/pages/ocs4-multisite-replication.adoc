= OpenShift Multisite Disaster Recovery
:toc:
:toclevels: 4
:icons: font
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:
:source-highlighter: pygments
:hide-uri-scheme:

== Overview

The intent of this guide is to detail the steps and commands necessary to be able to failover an application from one `OpenShift Container Platform` (OCP) cluster to another and then failback the same application to the primary cluster.

The necessary components are two OCP 4.6 (or greater) clusters, connectivity between their private networks, and `OpenShift Container Storage` (OCS) installed on both OCP clusters. OCS version *4.7* (or greater) is required for the `RBD Mirroring` feature to provide asynchronous storage replication. In order to also replicate the OCP metadata (pods, services, routes, etc.) from one cluster to another, this guide will make use of the Velero `Backup` and `Restore` APIs exposed via the OCP community operator `OpenShift APIs for Data Protection` or `OADP`.

=== OpenShift Multisite Connectivity

For this solution to work the OpenShift SDNs (Software Defined Networks) must be connected so that `OpenShift Container Storage` (OCS) resources can communicate, in particular the Ceph Monitors on the two different clusters must have connectivity. There are various ways to connect the private SDNs and the choice for this guide is to use `Submariner`.

IMPORTANT: Whatever way the OCP clusters are connected, the default `IP CIDRs` (Classless inter-domain routing) for cluster and service networking must be modified to be different on one of the clusters to avoid addressing conflicts.

`Submariner` consists of several main components that work in conjunction to securely connect workloads across multiple Kubernetes or OCP clusters, both on-premises and in public clouds. They are the following:

* Gateway Engine: manages the secure tunnels to other clusters.
* Route Agent: routes cross-cluster traffic from nodes to the active Gateway Engine.
* Broker: facilitates the exchange of metadata between Gateway Engines enabling them to discover one another.
* Service Discovery: provides DNS discovery of Services across clusters.

`Submariner` does support connecting  OCP clusters installed on AWS. There is also support for connecting non-AWS OCP clusters such as those installed on VMware or Bare Metal (BM). Hybrid connectivity of one OCP cluster on AWS and the 2nd cluster on non-AWS (i.e. VMware) infrastructure is also possible.

For more information on `Submariner` and the different configuration options go to (https://submariner.io/getting-started/).

==== Submariner Prerequisites

There are a few prerequisites to deploy `Submariner`. The first requirement is to modify the *install-config.yaml* configuration used with *openshift-install* before installing OpenShift so that the IP ranges for the cluster and service networks will be different as shown in examples below.

Example for `site1` *install-config.yaml*:

[source,yaml]
----
[...]
metadata:
  name: site1
networking:
  clusterNetwork:
  - cidr: 10.6.0.0/16       <1>
    hostPrefix: 23
  machineCIDR: 10.0.0.0/16
  networkType: OpenShiftSDN
  serviceNetwork:
  - 10.16.0.0/16            <2>
[...]
----

<1> clusterNetwork for site1
<2> serviceNetwork for site1

Example for `site2` *install-config.yaml*

[source,yaml]
----
[...]
metadata:
  name: site2
networking:
  clusterNetwork:
  - cidr: 10.12.0.0/16      <1>
    hostPrefix: 23
  machineCIDR: 10.0.0.0/16
  networkType: OpenShiftSDN
  serviceNetwork:
  - 10.112.0.0/16           <2>
[...]
----

<1> clusterNetwork for site2
<2> serviceNetwork for site2

If you already have your OCP clusters deployed, you can check your clusterNetwork and serviceNetwork configuration like this:

[source,role="execute"]
----
oc get networks.config.openshift.io cluster -o json | jq .spec
----
.Example output:
[source,json]
----
{
  "clusterNetwork": [
    {
      "cidr": "10.5.0.0/16",
      "hostPrefix": 23
    }
  ],
  "externalIP": {
    "policy": {}
  },
  "networkType": "OpenShiftSDN",
  "serviceNetwork": [
    "10.15.0.0/16"
  ]
}
----

===== On AWS clusters

For OCP on AWS the following packages needs to be available on your deploy host to install `Submariner`:

* AWS CLI
* Terraform version 0.12 (maximum version is 0.12.12)
* wget

You also need to download the `prep_for_subm.sh` script to install `Submariner` when one or both of your OCP clusters are on AWS.

[source,role="execute"]
----
curl https://raw.githubusercontent.com/submariner-io/submariner/devel/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O
----

Now make `prep_for_subm.sh` executable.

[source,role="execute"]
----
chmod a+x ./prep_for_subm.sh
----

Refer to these links for additional information about prerequisites when at least one OCP instance is installed on AWS.

* https://submariner.io/getting-started/quickstart/openshift/aws/
* https://submariner.io/getting-started/quickstart/openshift/vsphere-aws/

===== On non-AWS clusters

For non-AWS OCP clusters the only requirement is to download the `subctl` client that will be used for most `Submariner` commands.

===== On all platforms

Install the `subctl` to control the submariner installation

[source,role="execute"]
----
curl -Ls https://get.submariner.io | bash
----

And export the path to `subctl`:

[source,role="execute"]
----
export PATH=$PATH:~/.local/bin
----

NOTE: Add exported path for `subctl` in your deploy host `.profile` or `.bashrc`.

Once the two OCP clusters are created note the location of their unique `kubeconfig` (i.e.~/site1/auth/kubeconfig).

==== Submariner Installation

The `Submariner` installation detailed in this guide is for two non-AWS OCP clusters installed on VMware.

NOTE: Make sure to delete any prior `broker-info.subm` file before creating a new `broker-info.subm`.

NOTE: All `subctl` commands can be executed from any node that has network access to the API endpoint of both clusters

Start by deploying the `broker`.

[source,role="execute"]
----
subctl deploy-broker --kubeconfig site1/auth/kubeconfig
----
.Example output:
----
 ✓ Deploying broker
 ✓ Creating broker-info.subm file
 ✓ A new IPsec PSK will be generated for broker-info.subm
----

Now we want to create the connection between the two OCP clusters. The `gateway` *Pod* will be created on the node selected from the displayed list of available nodes during the `subctl join`.

NOTE: The `--disable-nat` flag is used when the connection between the two OCP clusters does not involve `NAT` (Network Address Translation). Reference https://submariner.io/operations/deployment/subctl/[Submariner documentation] for how to `subctl join` OCP clusters using `NAT`.

[source,role="execute"]
----
subctl join --kubeconfig site1/auth/kubeconfig --clusterid site1 broker-info.subm --disable-nat
----
.Example output:
----
* broker-info.subm says broker is at: https://api.site1.chris.ocs.ninja:6443
? Which node should be used as the gateway? site1-fqldq-worker-975qq
⢄⡱ Discovering network details     Discovered network details:
        Network plugin:  OpenShiftSDN
        Service CIDRs:   [10.16.0.0/16]
        Cluster CIDRs:   [10.6.0.0/16]
 ✓ Discovering network details
 ✓ Validating Globalnet configurations
 ✓ Discovering multi cluster details
 ✓ Deploying the Submariner operator
 ✓ Created operator CRDs
 ✓ Created operator namespace: submariner-operator
 ✓ Created operator service account and role
 ✓ Updated the privileged SCC
 ✓ Created lighthouse service account and role
 ✓ Updated the privileged SCC
 ✓ Created Lighthouse service accounts and roles
 ✓ Deployed the operator successfully
 ✓ Creating SA for cluster
 ✓ Deploying Submariner
 ✓ Submariner is up and running
----

Next, do a similar command for `site2`. The displayed list of available nodes for the `gateway` *Pod* will be those for the `site2` OCP instance.

[source,role="execute"]
----
subctl join --kubeconfig site2/auth/kubeconfig --clusterid site2 broker-info.subm --disable-nat
----
.Example output:
----
* broker-info.subm says broker is at: https://api.site1.chris.ocs.ninja:6443
? Which node should be used as the gateway? site2-lc8kr-worker-8j2qk
⢄⡱ Discovering network details     Discovered network details:
        Network plugin:  OpenShiftSDN
        Service CIDRs:   [10.112.0.0/16]
        Cluster CIDRs:   [10.12.0.0/16]
 ✓ Discovering network details
 ✓ Validating Globalnet configurations
 ✓ Discovering multi cluster details
 ✓ Deploying the Submariner operator
 ✓ Created operator CRDs
 ✓ Created operator namespace: submariner-operator
 ✓ Created operator service account and role
 ✓ Updated the privileged SCC
 ✓ Created lighthouse service account and role
 ✓ Updated the privileged SCC
 ✓ Created Lighthouse service accounts and roles
 ✓ Deployed the operator successfully
 ✓ Creating SA for cluster
 ✓ Deploying Submariner
 ✓ Submariner is up and running
----

On the `site1` OCP that you are logged into you can validate that the `Submariner` *Pods* are running. The same *Pods* should be `Running` in `site2` in the `submariner-operator` project.

[source,role="execute"]
----
oc get pods -n submariner-operator --kubeconfig site1/auth/kubeconfig
----
.Example output:
----
NAME                                            READY   STATUS    RESTARTS   AGE
submariner-gateway-kthdc                        1/1     Running   0          28m
submariner-lighthouse-agent-6c5755764-hjhsm     1/1     Running   0          27m
submariner-lighthouse-coredns-c4f7b6b8c-7nqxz   1/1     Running   0          27m
submariner-lighthouse-coredns-c4f7b6b8c-nt2rl   1/1     Running   0          27m
submariner-operator-6df7c9d659-9d9pm            1/1     Running   0          28m
submariner-routeagent-b476m                     1/1     Running   0          27m
submariner-routeagent-bchnj                     1/1     Running   0          27m
submariner-routeagent-glmlj                     1/1     Running   0          27m
submariner-routeagent-qgdps                     1/1     Running   0          27m
submariner-routeagent-sl2tr                     1/1     Running   0          27m
submariner-routeagent-smmdt                     1/1     Running   0          27m
----

The last step is to validate the connection between the two OCP clusters using a `subctl verify` command.

[source,role="execute"]
----
subctl verify site1/auth/kubeconfig site2/auth/kubeconfig --only connectivity --verbose
----
.Example output:
----
Performing the following verifications: connectivity
Running Suite: Submariner E2E suite
===================================
Random Seed: 1614875124
Will run 17 of 34 specs
[...]
------------------------------

Ran 11 of 34 Specs in 159.666 seconds
SUCCESS! -- 11 Passed | 0 Failed | 0 Pending | 23 Skipped
----

You can also verify the connectivity this way using site specific kubeconfig and `subctl show connections`.

[source,role="execute"]
----
subctl show connections --kubeconfig site1/auth/kubeconfig | grep connected
----
.Example output:
----
site2-lc8kr-worker-8j2qk        site2                   10.70.56.173    libreswan           10.112.0.0/16, 10.12.0.0/16             connected
----

And then using `site2` kubeconfig.

[source,role="execute"]
----
subctl show connections --kubeconfig site2/auth/kubeconfig | grep connected
----
.Example output:
----
site1-fqldq-worker-975qq        site1                   10.70.56.191    libreswan           10.16.0.0/16, 10.6.0.0/16               connected
----

Now that the two OCP instances have their `clusterNetwork` and `serviceNetwork` connected the next step is to install `OpenShift Container Storage` version *4.7* and configure storage replication or `RDB Mirroring`.

=== Configuring Multisite Storage Replication

Mirroring is configured on a per-pool basis within peer clusters and can be configured on a specific subset of images within the pool. The `rbd-mirror` daemon is responsible for pulling image updates from the remote peer cluster and applying them to the image within the local cluster.

==== OpenShift Container Storage Installation

In order to configure storage replication between the two OCP instances `OpenShift Container Storage` must be installed first. Documentation for the deployment can be found at https://access.redhat.com/documentation/en-us/red_hat_openshift_container_storage.

OCS deployment guides and instructions are specific to your infrastructure (i.e. AWS, VMware, BM, Azure, etc.). Install OCS version *4.7* or greater on both OCP clusters.

You can validate the successful deployment of the OCS on each OCP instance you with the following:

[source,role="execute"]
----
oc get storagecluster -n openshift-storage ocs-storagecluster -o jsonpath='{.status.phase}{"\n"}'
----

If result is `Ready` on `site1` and `site2` clusters you are ready to continue.

==== Configuring RBD Mirroring between OCS clusters

The next step will be to create the mirroring relationship between the two OCS clusters so the RBD volumes or images created using the Ceph RBD storageclass can be replicated from one OCP cluster to the other OCP cluster.

===== Enable OMAP Generator

Omap generator is a sidecar container that, when deployed with the CSI provisioner pod, generates the internal CSI omaps between the PV and the RBD image. The name of the new container is `csi-omap-generator`. This is required as static *PVs* are transferred across peer clusters in the DR use case, and hence is needed to preserve *PVC* to storage mappings.

Execute these steps on each of the OCP `site1` and `site2` clusters to enable the OMAP generator.

Edit the rook-ceph-operator-config configmap and add `CSI_ENABLE_OMAP_GENERATOR` set to true.

[source,role="execute"]
----
oc patch cm rook-ceph-operator-config -n openshift-storage --type json --patch  '[{ "op": "add", "path": "/data/CSI_ENABLE_OMAP_GENERATOR", "value": "true" }]'
----
.Example output:
----
configmap/rook-ceph-operator-config patched
----

Validate that there are now 7 sidecar containers and that the `csi-omap-generator` container is now running.

[source,role="execute"]
----
oc get pods -l app=csi-rbdplugin-provisioner -o jsonpath={.items[*].spec.containers[*].name}
----
.Example output:
----
csi-provisioner csi-resizer csi-attacher csi-snapshotter csi-omap-generator csi-rbdplugin liveness-prometheus csi-provisioner csi-resizer csi-attacher csi-snapshotter csi-omap-generator csi-rbdplugin liveness-prometheus
----

There are two `csi-rbdplugin-provisioner` pods for availability so there should be two groups of the same 7 containers for each pod.

IMPORTANT: Repeat these steps for the *remote cluster* or `site2` before proceeding and also repeat the validation for the new `csi-omap-generator` container.

===== Create Ceph Pools for replication

In this section you will create a new *CephBlockPool* that is RBD mirroring enabled. Execute the steps on each of the OCP clusters to enable mirroring and configure the `snapshot` schedule for images.

Sample Ceph block pool that has mirroring enabled:

[source,yaml]
----
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
   name: replicapool
   namespace: openshift-storage
spec:
   replicated:
     size: 3
   mirroring:
     enabled: true
     mode: image
       # specify the schedules on which snapshots should be taken
     snapshotSchedules:
       - interval: 1h
       #  startTime: 00:00:00-05:00
   statusCheck:
     mirror:
       disabled: false
       interval: 60
----

NOTE: The `snapshotSchedules` is a global value for the specific *CephBlockPool* used to configure the snapshot interval between peers for `mirror-enabled` volumes in this pool.

Now create new *CephBlockPool*.

[source,role="execute"]
----
curl -s https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/training/modules/ocs4/attachments/replicapool.yaml | oc apply -f -
----
.Example output:
----
cephblockpool.ceph.rook.io/replicapool created
----

IMPORTANT: Repeat the steps on the OCP *secondary cluster* or `site2`.

===== Bootstrap Peer Clusters

In order for the `rbd-mirror` daemon to discover its peer cluster, the peer must be registered and a user account must be created. The following steps enables `Bootstrapping` peers to discover and authenticate to each other.

For `Bootstrapping` a peer cluster it's bootstrap secret is required.

NOTE: Execute the following commands on the *secondary cluster* or `site2` cluster first.

To determine the name of the secret that contains the bootstrap secret do the following:

[source,role="execute"]
----
oc --kubeconfig site2/auth/kubeconfig get cephblockpool.ceph.rook.io/replicapool -n openshift-storage -ojsonpath='{.status.info.rbdMirrorBootstrapPeerSecretName}{"\n"}'
----
.Example output:
----
pool-peer-token-replicapool
----

The secret `pool-peer-token-replicapool` contains all the information related to the token and needs to be injected to the peer, to fetch the decoded secret do the following:

[source,role="execute"]
----
oc --kubeconfig site2/auth/kubeconfig get secrets pool-peer-token-replicapool -n openshift-storage -o jsonpath='{.data.token}' | base64 -d
----
.Example output:
----
eyJmc2lkIjoiNTliODkwMjEtM2VlMi00YTI1LWIwODctYjQzZWU4MGIzZGRlIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFCd3pVSmdGWVFySXhBQU5NR3AyK3BrR2hJa1dzMXBFclN4ZUE9PSIsIm1vbl9ob3N0IjoiW3YyOjEwLjExMi4yNDQuMTQ2OjMzMDAsdjE6MTAuMTEyLjI0NC4xNDY6Njc4OV0sW3YyOjEwLjExMi4xMDMuNTU6MzMwMCx2MToxMC4xMTIuMTAzLjU1OjY3ODldLFt2MjoxMC4xMTIuMTI0LjExOTozMzAwLHYxOjEwLjExMi4xMjQuMTE5OjY3ODldIn0=
----

Now get the site name for the *secondary cluster* or `site2` cluster:

[source,role="execute"]
----
oc --kubeconfig site2/auth/kubeconfig get cephblockpools.ceph.rook.io replicapool -n openshift-storage -o jsonpath='{.status.mirroringInfo.summary.summary.site_name}{"\n"}'
----
.Example output:
----
59b89021-3ee2-4a25-b087-b43ee80b3dde-openshift-storage
----

NOTE: Execute the following command on the *primary cluster* or `site1` cluster.

With the decoded value, create a secret on the *primary cluster*, using the site name of the *secondary cluster* from prior step as the secret name.

IMPORTANT: *Make sure to replace site name and token with the values from your clusters.*

----
oc --kubeconfig site1/auth/kubeconfig -n openshift-storage \
  create secret generic 59b89021-3ee2-4a25-b087-b43ee80b3dde-openshift-storage \
  --from-literal=token=eyJmc2lkIjoiNTliODkwMjEtM2VlMi00YTI1LWIwODctYjQzZWU4MGIzZGRlIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFCd3pVSmdGWVFySXhBQU5NR3AyK3BrR2hJa1dzMXBFclN4ZUE9PSIsIm1vbl9ob3N0IjoiW3YyOjEwLjExMi4yNDQuMTQ2OjMzMDAsdjE6MTAuMTEyLjI0NC4xNDY6Njc4OV0sW3YyOjEwLjExMi4xMDMuNTU6MzMwMCx2MToxMC4xMTIuMTAzLjU1OjY3ODldLFt2MjoxMC4xMTIuMTI0LjExOTozMzAwLHYxOjEwLjExMi4xMjQuMTE5OjY3ODldIn0= \
  --from-literal=pool=replicapool
----
.Example output:
----
secret/59b89021-3ee2-4a25-b087-b43ee80b3dde-openshift-storage created
----

This completes the bootstrap process for the *primary cluster* to the *secondary cluster* or site1 to site2.

NOTE: Repeat the process switching the steps for the *secondary cluster* and the *primary cluster*.

After generating the *primary cluster* site name and token the following example command is done on the *secondary cluster*.

IMPORTANT: *Make sure to replace site name and token with the values from your clusters.*

----
oc --kubeconfig site2/auth/kubeconfig -n openshift-storage \
  create secret generic dc12a67b-d82c-4b7c-b3d7-60a44d973772-openshift-storage \
  --from-literal=token=eyJmc2lkIjoiZGMxMmE2N2ItZDgyYy00YjdjLWIzZDctNjBhNDRkOTczNzcyIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFBY3pVSmdUTjZKT0JBQWlXZDNBV3UxeE41N1NrMVd0L3owaUE9PSIsIm1vbl9ob3N0IjoiW3YyOjEwLjE2LjIzOS40OjMzMDAsdjE6MTAuMTYuMjM5LjQ6Njc4OV0sW3YyOjEwLjE2LjE2My4xMzI6MzMwMCx2MToxMC4xNi4xNjMuMTMyOjY3ODldLFt2MjoxMC4xNi40NC44NTozMzAwLHYxOjEwLjE2LjQ0Ljg1OjY3ODldIn0= \
  --from-literal=pool=replicapool
----
.Example output:
----
secret/dc12a67b-d82c-4b7c-b3d7-60a44d973772-openshift-storage created
----

This completes the bootstrap process for the *secondary cluster* to the *primary cluster* or site2 to site1.

===== Create RBD Mirror Custom Resource

Replication is handled by the `rbd-mirror` daemon. The `rbd-mirror` daemon is responsible for pulling image updates from the *secondary cluster*, and applying them to images within the local cluster.

The `rbd-mirror` daemon(s) can be created using a custom resource (CR). There must be a `rbd-mirror` daemon or *Pod* created on the *primary cluster* and the *secondary cluster* using this CR:

[source,yaml]
----
apiVersion: ceph.rook.io/v1
kind: CephRBDMirror
metadata:
  name: rbd-mirror
  namespace: openshift-storage
spec:
  # the number of rbd-mirror daemons to deploy
  count: 1
  peers:
    secretNames:
      # list of Kubernetes Secrets containing the peer token
      - SECRET  # <-- Fill in correct value
    resources:
      # The pod requests and limits
      limits:
        cpu: "1"
        memory: "2Gi"
      requests:
        cpu: "1"
        memory: "2Gi"
----

To get the `secret` for the *primary cluster* or site1 do the following:

[source,role="execute"]
----
SECRET=$(oc get secrets | grep openshift-storage | awk {'print $1}')
echo $SECRET
----
.Example output:
----
59b89021-3ee2-4a25-b087-b43ee80b3dde-openshift-storage
----

Now create the `rbd-mirror` *Pod* for the *primary site*:

[source,role="execute"]
----
curl -s https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/training/modules/ocs4/attachments/rbd-mirror.yaml | sed -e "s/SECRET/${SECRET}/g" | oc apply -f -
----
.Example output:
----
cephrbdmirror.ceph.rook.io/rbd-mirror created
----

Check to see if the new`rbd-mirror` *Pod* is created and `Running`.

[source,role="execute"]
----
oc get pods -n openshift-storage | grep rbd-mirror
----
.Example output:
----
rook-ceph-rbd-mirror-a-57ccc68d88-lts87                           2/2     Running     0          5m
----

Check the status of the `rbd-mirror` daemon health.

[source,role="execute"]
----
oc get cephblockpools.ceph.rook.io replicapool -n openshift-storage -o jsonpath='{.status.mirroringStatus.summary.summary}{"\n"}'
----
.Example output:
----
{"daemon_health":"OK","health":"OK","image_health":"OK","states":{}}
----

Now repeat process for *secondary cluster* or `site2`.

IMPORTANT: Make sure to do all steps above on the *secondary cluster*. The results for `SECRET` should be different than the *primary cluster* as a way to check you are on the *secondary cluster*.

You have now completed the steps for configuring *RBD Mirroring* between the *primary cluster* and the *secondary cluster* or site1 and site2. The next sections will cover how to enable Ceph RBD images (volumes) for mirroring data from site1 to site2 asynchronously. Also, using a sample application, detailed instructions will be provided on how to `failover` from site1 to site2 and how to `failback` the application all the while preserving the persistent data.

=== Creating Mirror storage class and Sample application

==== Storageclass for volume replication

Before any new OCS volumes are created for replication a new *StorageClass* needs to be created using *CephBlockPool* replicapool that was created in prior section. Also, the `reclaimPolicy` needs to be `Retain` verses `Delete` as it for the OCS default *StorageClasses*.

Example *StorageClass*:

[source,yaml]
----
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ocs-storagecluster-ceph-mirror
parameters:
  clusterID: openshift-storage
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
  csi.storage.k8s.io/fstype: ext4
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
  imageFeatures: layering
  imageFormat: "2"
  pool: replicapool
provisioner: openshift-storage.rbd.csi.ceph.com
reclaimPolicy: Retain
volumeBindingMode: Immediate
----

Now create the *StorageClass*:

[source,role="execute"]
----
curl -s https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/training/modules/ocs4/attachments/ocs-storagecluster-ceph-mirror.yaml | oc apply -f -
----
.Example output:
----
storageclass.storage.k8s.io/ocs-storagecluster-ceph-mirror created
----

IMPORTANT: Make sure to also create the `ocs-storagecluster-ceph-mirror` *StorageClass* on the *secondary cluster* before proceeding.

==== Sample Application

In order to test failing over from one OCP cluster to another we need a simple application to and verify that replication is working.

Start by creating a new project on the *primary cluster*:

[source,role="execute"]
----
oc new-project my-database-app
----

Then use the `rails-pgsql-persistent` template to create the new application. The new `postgresql` volume will be claimed from the new *StorageClass*.

[source,role="execute"]
----
curl -s https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/training/modules/ocs4/attachments/configurable-rails-app.yaml | oc new-app -p STORAGE_CLASS=ocs-storagecluster-ceph-mirror -p VOLUME_CAPACITY=5Gi -f -
----

After the deployment is started you can monitor with these commands.

[source,role="execute"]
----
oc status
----

Check the PVC is created.

[source,role="execute"]
----
oc get pvc -n my-database-app
----

This step could take 5 or more minutes. Wait until there are 2 *Pods* in
`Running` STATUS and 4 *Pods* in `Completed` STATUS as shown below.

[source,role="execute"]
----
watch oc get pods -n my-database-app
----
.Example output:
----
NAME                                READY   STATUS      RESTARTS   AGE
postgresql-1-deploy                 0/1     Completed   0          5m48s
postgresql-1-lf7qt                  1/1     Running     0          5m40s
rails-pgsql-persistent-1-build      0/1     Completed   0          5m49s
rails-pgsql-persistent-1-deploy     0/1     Completed   0          3m36s
rails-pgsql-persistent-1-hook-pre   0/1     Completed   0          3m28s
rails-pgsql-persistent-1-pjh6q      1/1     Running     0          3m14s
----

You can exit by pressing kbd:[Ctrl+C].

Once the deployment is complete you can now test the application and the
persistent storage on Ceph.

[source,role="execute"]
----
oc get route rails-pgsql-persistent -n my-database-app -o jsonpath --template="http://{.spec.host}/articles{'\n'}"
----

This will return a route similar to this one.

.Example output:
----
http://rails-pgsql-persistent-my-database-app.apps.cluster-ocs4-8613.ocs4-8613.sandbox944.opentlc.com/articles
----

Copy your route (different than above) to a browser window to create articles.

Enter the `username` and `password` below to create articles and comments.
The articles and comments are saved in a PostgreSQL database which stores its
table spaces on the Ceph RBD volume provisioned using the
`ocs-storagecluster-ceph-rbd` *StorageClass* during the application
deployment.

----
username: openshift
password: secret
----

Once you have added a new article you can verify it exists in the `postgresql` database by issuing this command:

[source,role="execute"]
----
oc rsh -n my-database-app $(oc get pods -n my-database-app|grep postgresql | grep -v deploy | awk {'print $1}') psql -c "\c root" -c "\d+" -c "select * from articles"
----
.Example output:
----
You are now connected to database "root" as user "postgres".
                               List of relations
 Schema |         Name         |   Type   |  Owner  |    Size    | Description 
--------+----------------------+----------+---------+------------+-------------
 public | ar_internal_metadata | table    | userXQR | 16 kB      | 
 public | articles             | table    | userXQR | 16 kB      | 
 public | articles_id_seq      | sequence | userXQR | 8192 bytes | 
 public | comments             | table    | userXQR | 8192 bytes | 
 public | comments_id_seq      | sequence | userXQR | 8192 bytes | 
 public | schema_migrations    | table    | userXQR | 16 kB      | 
(6 rows)

 id |     title     |                  body                  |         created_a
t         |         updated_at         
----+---------------+----------------------------------------+------------------
----------+----------------------------
  2 | First Article | This is article #1 on primary cluster. | 2021-03-19 22:05:
07.255362 | 2021-03-19 22:05:07.255362
(1 row)
----

=== Installing OADP for OpenShift metadata

OADP (OpenShift APIs for Data Protection) is a community operator and is available in *OperatorHub*. 

We will be using OADP for the `Backup` and `Restore` APIs for collecting the OpenShift metadata at a namespace level. This metadata collection is needed to restore the application on the *secondary cluster*.

==== Installing OADP from OperatorHub

First is to find OADP in *OperatorHub*. Login to your *OpenShift Web Console* and navigate to *OperatorHub*. Filter for `OADP` as shown below:

.OperatorHub filter for OADP
image::OCP4-OADP-operatorhub-filter.png[OperatorHub filter for OADP]

NOTE: If you are not finding OADP in *OperatorHub* most likely the `community-operator` catalogsource is not deployed in your cluster.

Select `Continue` on next screen.

.OADP operator support statement
image::OCP4-OADP-operatorhub-continue.png[OADP operator support statement]

CAUTION: OADP is a community operator and as such is not supported by Red Hat. More information can be found at https://github.com/konveyor/oadp-operator.

Select `Install` on next screen.

.OADP install screen
image::OCP4-OADP-operatorhub-install.png[OADP install screen]

Now you will create the new namespace `oadp-operator` and install the OADP operator into this namespace. Select `Install` again.

.OADP create namespace and install operator
image::OCP4-OADP-operatorhub-install2.png[OADP create namespace and install operator]

Wait for operator to install. When you see this screen the OADP operator is installed.

.OADP operator installed and ready
image::OCP4-OADP-operator-installed.png[OADP operator installed and ready]

The next step is to create the `Velero` *CustomResource* or CR. For this you will need to have a `S3` compatible object bucket created that you know the `bucket name` as well as the credentials to access the bucket.

NOTE: It is not recommended to use OCS object buckets (MCG or RGW) as the `S3` *BackingStorageLocation* for `Velero` CR. If your remote or secondary clusters become unavailable and the `S3` bucket is created on that cluster there is no way to recover to alternate cluster.

==== Creating bucket credentials secret

Before creating the  `Velero` CR you must create the `cloud-credentials` file with the creditials for your `S3` bucket. The format of the file needs to be this:

----
[default]
aws_access_key_id=VELERO_ACCESS_KEY_ID
aws_secret_access_key=VELERO_SECRET_ACCESS_KEY
----

Copy your unique credentials into file `cloud-credentials` and save file.

Now use this new `cloud-credentials` file to create a new *Secret*. Replace `<CREDENTIALS_FILE_PATH>` with path to file you created with `S3` credentials.

----
oc create secret generic cloud-credentials --namespace oadp-operator --from-file cloud=<CREDENTIALS_FILE_PATH>/cloud-credentials
----

==== Creating Velero resource

The velero YAML file needs to be modified to be correct for your `S3` bucket. The example is for a `S3` bucket on AWS saved as file `velero-aws.yaml`.

[source,yaml]
----
apiVersion: konveyor.openshift.io/v1alpha1
kind: Velero
metadata:
  name: oadp-velero
  namespace: oadp-operator
spec:
  olm_managed: true
  backup_storage_locations:
    - config:
        profile: default
        region: us-east-2  # <-- Modify to your AWS region
      credentials_secret_ref:
        name: cloud-credentials
        namespace: oadp-operator
      name: default
      object_storage:
        bucket: oadp-xxxxxx # Modify to your AWS bucket name
        prefix: velero
      provider: aws
  default_velero_plugins:
    - aws
    - openshift
  enable_restic: false
----

Once you have your unique values copied into your YAML file create the `Velero` CR.

NOTE: If wanting to us a MCG or RGW `S3` object bucket instead of a bucket off platform (i.e. AWS) as recommended, reference these instructions https://github.com/konveyor/oadp-operator/blob/master/docs/noobaa/install_oadp_noobaa.md.

----
oc create -f velero-aws.yaml -n oadp-operator
----

Validate that the `velero` pod is `Running` and that the *BackingStorageLocation* have been created as well that has the details to access your `S3` bucket for OpenShift metadata storage.

[source,role="execute"]
----
oc get pods,backupstoragelocation -n oadp-operator
----
.Example output:
----
NAME                                           READY   STATUS    RESTARTS   AGE
pod/oadp-default-aws-registry-88f556c5-2mk6h   1/1     Running   0          4m59s
pod/oadp-operator-6bb9fb6cfc-mc6vw             1/1     Running   0          49m
pod/velero-6c6fd6d84d-mbct9                    1/1     Running   0          5m3s

NAME                                      PHASE       LAST VALIDATED   AGE
backupstoragelocation.velero.io/default   Available   9s               5m1s
----

IMPORTANT: Repeat these steps and install *OADP* on the *secondary cluster*. Make sure to use the same `S3` bucket and credentials as for the *primary cluster* when creating the `Velero` CR.

=== Failover to Secondary cluster

The setup and configuration steps in the prior section have prepared the environment to support a failover event from the *primary cluster* to the *secondary cluster*. In our case this will be for just one namespace (my-database-app) that includes restoring the OpenShift metadata and persistent data stored in *PVCs*. The following steps will be followed for the failover:
[start=1]
. Using the toolbox enable image(s) for replication via snapshot to peer cluster.
. Use OADP and the `Backup` CR to collect OpenShift metadata for application namespace.
. Scale application deployment down to take application offline.
. Using the `toolbox` *demote* the storage for the application on the *primary cluster*.
. Using the `toolbox` *promote* the storage on the *secondary cluster*.
. Use OADP and the `Restore` CR to bring the application online.
. Verify use of the application on the *secondary cluster*.

==== Installing the toolbox for Ceph commands

Since the Rook-Ceph *toolbox* is not shipped with OCS you will need to deploy it
manually because a few steps of the failover process require use of Ceph commands today.

You can patch the `OCSInitialization ocsinit` to create the *toolbox* using the following command line:

[source,role="execute"]
----
oc patch OCSInitialization ocsinit -n openshift-storage --type json --patch  '[{ "op": "replace", "path": "/spec/enableCephTools", "value": true }]'
----

After the `rook-ceph-tools` *Pod* is `Running` you can access the *toolbox*
like this:

[source,role="execute"]
----
TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD
----

Once inside the *toolbox* try out the following Ceph commands:

Check the health of the Ceph cluster first.

[source,role="execute"]
----
ceph health
----
.Example output:
----
HEALTH_OK
----

CAUTION: Make sure that `HEALTH_OK` is the status before proceeding.

[source,role="execute"]
----
rbd -p replicapool mirror pool status
----
.Example output:
----
health: OK
daemon health: OK
image health: OK
images: 0 total
----

[source,role="execute"]
----
rbd -p replicapool mirror snapshot schedule ls
----
.Example output:
----
every 1h
----

You can exit the toolbox by either pressing kbd:[Ctrl+D] or by executing exit.

[source,role="execute"]
----
exit
----

NOTE: Make sure to repeat these steps on the *secondary cluster* as well and logon to the *toolbox* and run the same Ceph commands to validate the health of the cluster and mirroring.

==== Enable images for snapshot replication

In order to have persistent data replicated for a particular application the volume(s) have to be enabled for mirroring. This is currently done using Ceph commands after logging into the *toolbox*.

Login to the *toolbox* again on the *primary cluster* if not already in the *toolbox*.

[source,role="execute"]
----
TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD
----

List the images in the *CephBlockPool* replicapool.

[source,role="execute"]
----
rbd -p replicapool ls
----
.Example output:
----
csi-vol-94953897-88fc-11eb-b175-0a580a061092
----

In this case there is only one image or volume that was created for the `postgresql` persistent data storage. This is the image you want to enable for mirroring on the *primary cluster*.

IMPORTANT: Your image name will be different. Use your image name for following commands.

[source,role="execute"]
----
rbd -p replicapool mirror image enable csi-vol-94953897-88fc-11eb-b175-0a580a061092 snapshot
----
.Example output:
----
Mirroring enabled
----

You can now get more information about image mirroring by doing this command on the *primary cluster*,

[source,role="execute"]
----
rbd -p replicapool info csi-vol-94953897-88fc-11eb-b175-0a580a061092
----
.Example output from *primary cluster*:
----
rbd image 'csi-vol-94953897-88fc-11eb-b175-0a580a061092':
	size 5 GiB in 1280 objects
	order 22 (4 MiB objects)
	snapshot_count: 1
	id: ee409072562b
	block_name_prefix: rbd_data.ee409072562b
	format: 2
	features: layering
	op_features: 
	flags: 
	create_timestamp: Fri Mar 19 21:46:38 2021
	access_timestamp: Fri Mar 19 21:46:38 2021
	modify_timestamp: Fri Mar 19 21:46:38 2021
	mirroring state: enabled
	mirroring mode: snapshot
	mirroring global id: 8cd6c7e8-a92b-4d1c-bcac-d9c9cd234980
	mirroring primary: true  <1>
----
<1> Currently storage is promoted on *primary cluster*

To validate the mirroring or replication is working you can logon to the *toolbox* on the *secondary cluster* and run the same command for the exact same image name which should now be replicated to the peer cluster.

[source,role="execute"]
----
rbd -p replicapool info csi-vol-94953897-88fc-11eb-b175-0a580a061092
----
.Example output from *secondary cluster*:
----
rbd image 'csi-vol-94953897-88fc-11eb-b175-0a580a061092':
	size 5 GiB in 1280 objects
	order 22 (4 MiB objects)
	snapshot_count: 1
	id: 74c39ad8d17a
	block_name_prefix: rbd_data.74c39ad8d17a
	format: 2
	features: layering, non-primary
	op_features: 
	flags: 
	create_timestamp: Sun Mar 21 00:49:58 2021
	access_timestamp: Sun Mar 21 00:49:58 2021
	modify_timestamp: Sun Mar 21 00:49:58 2021
	mirroring state: enabled
	mirroring mode: snapshot
	mirroring global id: 8cd6c7e8-a92b-4d1c-bcac-d9c9cd234980
	mirroring primary: false  <1>
----
<1> Currently storage is demoted on *secondary cluster*

These steps would be repeated for every image that you want to mirror via snapshot to the peer cluster. For this example the snapshot interval is `1 hour` and was configured in the `replicapool` *CephBlockPool* CR.

You can exit the toolbox by either pressing kbd:[Ctrl+D] or by executing exit.

[source,role="execute"]
----
exit
----
 
==== Creating OpenShift metadata backup

The OpenShift metadata or resources have to be backup up and stored in a location where the *secondary cluster* can access. In this case using the `OADP` or `Velero` *Backup* API is how this will be done. 

Here is a sample `backup.yaml` file for the sample application:

[source,yaml]
----
apiVersion: velero.io/v1
kind: Backup
metadata:
  namespace: oadp-operator
  name: backup1
spec:
  includedNamespaces:
  - my-database-app
  snapshotVolumes: false
----  

Given the persistent data is going to be mirrored or replicated from the *primary cluster* to the *secondary cluster* we do not need the `OADP` *Backup* to include this data and therefore set `snapshotVolumes: false`.

Now create the *Backup* for `my-database-app` namespace.

[source,role="execute"]
----
curl -s https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/training/modules/ocs4/attachments/backup.yaml | oc apply -f -
----
----
.Example output:
----
backup.velero.io/backup1 created

Verify the *Backup* completed successfully to your `S3` object bucket target using the following command:

[source,role="execute"]
----
oc describe backup backup1 -n oadp-operator
----
.Example output:
----
Name:         backup1
Namespace:    oadp-operator
Labels:       velero.io/storage-location=default
Annotations:  velero.io/source-cluster-k8s-gitversion: v1.20.0+bd9e442
              velero.io/source-cluster-k8s-major-version: 1
              velero.io/source-cluster-k8s-minor-version: 20
API Version:  velero.io/v1
Kind:         Backup

[...]
Spec:
  Default Volumes To Restic:  false
  Included Namespaces:
    my-database-app. <1>
  Snapshot Volumes:  false
  Storage Location:  default
  Ttl:               720h0m0s
Status:
  Completion Timestamp:  2021-03-20T00:42:22Z
  Expiration:            2021-04-19T00:40:18Z
  Format Version:        1.1.0
  Phase:                 Completed. <2>
  Progress:
    Items Backed Up:  100 <3>
    Total Items:      100
  Start Timestamp:    2021-03-20T00:40:18Z
  Version:            1
Events:               <none>
----
<1> Namespace for which resources copied to object bucket
<2> Successul backup with Completed status
<3> The number of OpenShift resources backed up

==== Scaling application down on primary cluster

In order to simulate a Disaster Recovery (DR) event the application deployment will be scaled to zero. 

Let's take a look at the *DeploymentConfig* for our application.

[source,role="execute"]
----
oc get deploymentconfig -n my-database-app
----
.Example output:
----
NAME                     REVISION   DESIRED   CURRENT   TRIGGERED BY
postgresql               1          1         1         config,image(postgresql:10)
rails-pgsql-persistent   1          1         1         config,image(rails-pgsql-persistent:latest)
----

There are two *DeploymentConfig* to scale to zero.

[source,role="execute"]
----
oc scale deploymentconfig postgresql -n my-database-app --replicas=0
----
.Example output:
----
deploymentconfig.apps.openshift.io/postgresql scaled
----

Now scale the second deployment to zero.

[source,role="execute"]
----
oc scale deploymentconfig rails-pgsql-persistent -n my-database-app --replicas=0
----
.Example output:
----
deploymentconfig.apps.openshift.io/rails-pgsql-persistent scaled
----

Check to see the *Pods* are deleted. The following command should return *_no_* results if both *DeploymentConfig* are scaled to zero.

[source,role="execute"]
----
oc get pods -n my-database-app | grep Running
----

Test that the application is down on the *primary cluster* by refreshing the route in your browser or get route again and copy to browser tab.

[source,role="execute"]
----
oc get route rails-pgsql-persistent -n my-database-app -o jsonpath --template="http://{.spec.host}/articles{'\n'}"
----

You show see something like this now.

.Sample application is offline
image::sample-app-down-primary.png[Sample application is offline]

==== Demoting and Promoting storage to alternate site

In order to failover the storage on the *primary cluster* must be `demoted` and the storage on the *secondary cluster* must be `promoted. This is currently done on a per image basis using the *toolbox*. 

Logon to the *toolbox* on the *primary cluster* to use Ceph commands.

[source,role="execute"]
----
TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD
----

First `demote` the `postgresql` image on the *primary cluster*.

IMPORTANT: Your image name will be different. Use your image name for following commands.

[source,role="execute"]
----
rbd -p replicapool mirror image demote csi-vol-94953897-88fc-11eb-b175-0a580a061092
----
.Example output:
----
Image demoted to non-primary
----

Now logoon to the *toolbox* on the *secondary cluster* and `promote` the `postgresql` image.

[source,role="execute"]
----
rbd -p replicapool mirror image promote csi-vol-94953897-88fc-11eb-b175-0a580a06109
----
.Example output:
----
Image promoted to primary
----

Using the *toolbox* on the *secondary cluster* validate the image is now `promoted`.

[source,role="execute"]
----
rbd -p replicapool info csi-vol-94953897-88fc-11eb-b175-0a580a061092
----
.Example output from *secondary cluster*:
----
rbd image 'csi-vol-94953897-88fc-11eb-b175-0a580a061092':
	size 5 GiB in 1280 objects
	order 22 (4 MiB objects)
	snapshot_count: 1
	id: 74c39ad8d17a
	block_name_prefix: rbd_data.74c39ad8d17a
	format: 2
	features: layering
	op_features: 
	flags: 
	create_timestamp: Sun Mar 21 00:49:58 2021
	access_timestamp: Sun Mar 21 00:49:58 2021
	modify_timestamp: Sun Mar 21 00:49:58 2021
	mirroring state: enabled
	mirroring mode: snapshot
	mirroring global id: 8cd6c7e8-a92b-4d1c-bcac-d9c9cd234980
	mirroring primary: true  <1>
----
<1> Image is now promoted on *secondary cluster*

These steps would be repeated for every image that you want to `demote` and `promote` to the *secondary cluster*.

You can exit the toolbox by either pressing kbd:[Ctrl+D] or by executing exit.

[source,role="execute"]
----
exit
----

==== Restoring application to alternate cluster

The last step in the process to failover to the *secondary cluster* or alternate cluster is to now use `OADP` and the *Restore* CR to copy the OpenShift metadata to the alternate cluster. The persistent data is already `mirrored` to the *secondary cluster* from the *primary cluster* and therefore does not need to be copied.

Here is a sample `restore.yaml` file for the sample application:

[source,yaml]
----
apiVersion: velero.io/v1
kind: Restore
metadata:
  namespace: oadp-operator
  name: restore1
spec:
  backupName: backup1
  includedNamespaces:
  - my-database-app
----

Now create the *Restore* on the *secondary cluster* for the `my-database-app` namespace. You notice in the *Restore* that the `backup1` created earlier is referenced.

IMPORTANT: Make sure to issue this command on the *secondary cluster* where you are creating the `my-database-app` namespace and the OpenShift resources.

[source,role="execute"]
----
curl -s https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/training/modules/ocs4/attachments/restore.yaml | oc apply -f -
----
.Example output:
----
restore.velero.io/restore1 created
----

Verify the *Restore* completed successfully from your `S3` object bucket target using the following command:

[source,role="execute"]
----
oc describe restore restore1 -n oadp-operator
----
.Example output:
----
TBD
----

Now check to see that the *PODs* and *PVC* are created correctly ini `my-database-app`namespace on *secondary cluster*.

[source,role="execute"]
----
oc get pods,pvc -n my-database-app
----
.Example output:
----
TBD
----

==== Verifying application
