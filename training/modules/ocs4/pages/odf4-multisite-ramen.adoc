= OpenShift Regional Disaster Recovery with Advanced Cluster Management
:toc:
:toclevels: 4
:icons: font
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:
:source-highlighter: pygments
:hide-uri-scheme:

== Overview

The intent of this guide is to detail the steps and commands necessary to be able to failover an application from one `OpenShift Container Platform` (OCP) cluster to another and then failback the same application to the original *primary cluster*. In this case the OCP clusters will be created or imported using *Red Hat Advanced Cluster Management* or `RHACM`. 

This is a general overview of the steps required to configure and execute `Regional Disaster Recovery` capabilities using OpenShift Data Foundation (ODF) *v4.9* and `RHACM` *v2.4* across two distinct OCP clusters separated by distance. In addition to these two cluster called `managed` clusters, there is currently a requirement to have a third OCP cluster that will be the `Advanced Cluster Management` (ACM) `hub` cluster.

NOTE: These steps are considered Tech Preview in ODF 4.9 and are provided for POC (Proof of Concept) purposes. OpenShift `Regional Disaster Recovery` will be supported for production usage in a future ODF release.

[start=1]
. *Install the ACM operator on the hub cluster.* +
After creating the OCP hub cluster, install from OperatorHub the ACM operator. After the operator and associated pods are running, create the MultiClusterHub resource.
. *Create or import managed OCP clusters into ACM hub.* +
Import or create the two managed clusters with adequate resources (compute nodes, memory, cpu) to deploy ODF 4.9 using the RHACM console.
. *Ensure clusters have unique private network address ranges.* +
Ensure the primary and secondary OCP clusters have unique private network address ranges.
. *Connect the private networks using Submariner add-ons.* +
Connect the managed OCP private networks (cluster and service) using the RHACM Submariner add-ons.
. *Install ODF 4.9 on managed clusters.* +
Install ODF 4.9 on primary and secondary OCP managed clusters and validate deployment.
. *Create new CSI side-cars in each managed OCP cluster.* +
Using CLI, manually add the volume-replication and omap-generator side-car containers to rbd-provisioning pod(s).
. *[Pre-release] Create custom ODF catalogsource on ACM hub cluster.* +
Using CLI, manually create ocs-catalogsource in order to install ODF 4.9 pre-release builds.
. *Install ODF Multicluster-orchestrator operator on ACM hub cluster.* +
Using OperatorHub on ACM hub cluster install the multicluster orchestrator operator.
. *Install Mirror Peer resource on ACM hub cluster.* +
Using the multicluster orchestrator operator install the MirrorPeer resource using CLI or the operator wizard.
. *Enable Ceph mirroring on managed OCP clusters.* +
Using CLI, patch the ODF StorageCluster on each managed cluster to enable Ceph RBD mirroring.
. *Validate Ceph mirroring is active between managed OCP clusters.* +
Using CLI, validate the new rbd-mirroring pods are created in each managed cluster and that the default CephBlockPool has healthy mirroring status in both directions.
. *Create VolumeReplicationClass resource.* +
Using CLI, create the VolumeReplicationClass to configure the replication schedule (i.e. replicate between peers every 5 minutes).
. *Install ODR Cluster operator on managed clusters.* +
Install from OperatorHub on both managed clusters the ODR Cluster operator.
. *Configure S3 endpoints, Secrets and ConfigMaps.* +
After extracting the access/secret keys and S3 endpoint from the Multi-Cloud Gateway (MCG), create the required secrets and configmaps.

== Install and Configure ACM for Multisite connectivity

This installation method requires you have three OpenShift clusters that have network reachability between them. For the purposes of this document we will use this reference for the clusters:

* *Hub cluster* is where ACM, ODF Multisite-orchestrator and ODR Hub controllers are installed.
* *Primary managed cluster* is where ODF, ODR Cluster controller, and Applications are installed.
* *Secondary managed cluster* is where ODF, ODR Cluster controller, and Applications are installed.

Find ACM in OperatorHub on the *Hub cluster* and follow instructions to install this operator.

.OperatorHub filter for Advanced Cluster Management
image::ACM-OperatorHub.png[OperatorHub filter for Advanced Cluster Management]

Verify that the operator was successfully installed and that the `MultiClusterHub` is ready to be installed.

.ACM Installed Operator
image::ACM-Installed-Operator.png[ACM Installed Operator]

Select `MultiClusterHub` and use either `Form view` or `YAML view` to configure the deployment and select `Create`. 

NOTE: Most *MultiClusterHub* deployments can use default settings in the `Form view`. The only non-default setting to select in the `Form view` Advanced configuration is *Basic* in the Availability Configuration section.

Once the deployment is complete you can logon to the ACM console using your OpenShift credentials.

First, find the *Route* that has been created for the ACM console:

[source,role="execute"]
----
oc get route multicloud-console -n open-cluster-management -o jsonpath --template="http://{.spec.host}/multicloud/clusters{'\n'}"
----

This will return a route similar to this one.

.Example Output:
----
http://multicloud-console.apps.perf3.chris.ocs.ninja/multicloud/clusters
----

After logging in you should see your local cluster imported.

.ACM local cluster imported
image::ACM-local-cluster-import.png[ACM local cluster imported]

=== Import or Create Managed clusters

Now that ACM is installed on the `Hub cluster` it is time to either create or import the `Primary managed cluster` and the `Secondary managed cluster`. You should see selections (as in above diagram) for *Create cluster* and *Import cluster*. Chose the selection appropriate for your environment. After the managed clusters are successfully created or imported you should see something similar to below.

.ACM managed cluster imported
image::ACM-managed-clusters-import.png[ACM managed cluster imported]

=== Verify Managed clusters have non-overlapping networks

In order to connect the OpenShift cluster and service networks using the `Submariner add-ons`, it is necessary to validate the two clusters have non-overlapping networks. This can be done by running the following command for each of the managed clusters.

[source,role="execute"]
----
oc get networks.config.openshift.io cluster -o json | jq .spec
----
.Example output for ocp4perf1:
[source,json]
----
{
  "clusterNetwork": [
    {
      "cidr": "10.5.0.0/16",
      "hostPrefix": 23
    }
  ],
  "externalIP": {
    "policy": {}
  },
  "networkType": "OpenShiftSDN",
  "serviceNetwork": [
    "10.15.0.0/16"
  ]
}
----

.Example output for ocp4perf2:
[source,json]
----
{
  "clusterNetwork": [
    {
      "cidr": "10.6.0.0/16",
      "hostPrefix": 23
    }
  ],
  "externalIP": {
    "policy": {}
  },
  "networkType": "OpenShiftSDN",
  "serviceNetwork": [
    "10.16.0.0/16"
  ]
}
----

These outputs show that the two example managed clusters have non-overlapping `clusterNetwork` and `serviceNetwork` ranges so it is safe to proceed.

=== Connect the Managed clusters using Submariner add-ons

Now that we know the `cluster` and `service` networks have non-overlapping ranges, it is time to move on to installing the `Submariner add-ons` for each managed cluster. This is done by using the ACM console and `Cluster sets`.

Navigate to selection shown below and at the bottom of the same page, select *Create cluster set*.

.ACM Create new Cluster set
image::ACM-Cluster-sets.png[ACM Create new Cluster set]

Once the new `Cluster set` is created select *Manage resource assignments*.

.ACM Cluster set created
image::ACM-Cluster-set-created.png[ACM Cluster set created]

Follow the instructions and add the two managed clusters to the new `Cluster set`. Select `Save` and then navigate to `Submariner add-ons`.

.ACM Submariner add-ons
image::ACM-Submariner-addon.png[ACM Submariner add-ons]

Select *Install Submariner add-ons* at the bottom of the page and add the two managed clusters. Click through the wizard selections and make changes as needed. After `Review` of your selections select *Install*.

IMPORTANT: It can take more than 5 minutes for the Submariner add-ons installation to finish on both managed clusters. Resources are installed in the `submariner-operator` project.

A successful deployment will show `Connection status` and `Agent status` as `Healthy`.

.ACM Submariner add-ons installed
image::ACM-Submariner-addon-installed.png[ACM Submariner add-ons installed]

== OpenShift Data Foundation Installation

In order to configure storage replication between the two OCP clusters `OpenShift Data Foundation` (ODF) must be installed first on each managed cluster. ODF deployment guides and instructions are specific to your infrastructure (i.e. AWS, VMware, BM, Azure, etc.). Install ODF version *4.9* or greater on both OCP managed clusters.

You can validate the successful deployment of ODF on each managed OCP cluster with the following command:

[source,role="execute"]
----
oc get storagecluster -n openshift-storage ocs-storagecluster -o jsonpath='{.status.phase}{"\n"}'
----

If the result is `Ready` on the *Primary managed cluster* and the *Secondary managed cluster* continue on to configuring mirroring.

NOTE: The successful installation of ODF can also be validated in the *OCP Web Console* by navigating to *Storage* and then *Overview*. The `Block and File` and `Object` dashboards should show green status. 

== Configuring Multisite Storage Replication

Mirroring or replication is enabled on a per `CephBlockPool` basis within peer managed clusters and can then be configured on a specific subset of images within the pool. The `rbd-mirror` daemon is responsible for replicating image updates from the local peer cluster to the same image in the remote cluster.

These instructions detail how to create the mirroring relationship between two ODF managed clusters.

=== Enable OMAP Generator & Volume Replication on the managed clusters

Execute the following steps on the *Primary managed cluster* and the *Secondary managed cluster* to enable the OMAP and Volume-Replication CSI sidecar containers in the `csi-rbdplugin-provisioner` *Pods*.

Edit the `rook-ceph-operator-config` *ConfigMap* and add `CSI_ENABLE_OMAP_GENERATOR` set to true.

[source,role="execute"]
----
oc patch cm rook-ceph-operator-config -n openshift-storage --type json --patch  '[{ "op": "add", "path": "/data/CSI_ENABLE_OMAP_GENERATOR", "value": "true" }]'
----
.Example output.
----
configmap/rook-ceph-operator-config patched
----

Edit the `rook-ceph-operator-config` *ConfigMap* and add `CSI_ENABLE_VOLUME_REPLICATION` set to true.

[source,role="execute"]
----
oc patch cm rook-ceph-operator-config -n openshift-storage --type json --patch  '[{ "op": "add", "path": "/data/CSI_ENABLE_VOLUME_REPLICATION", "value": "true" }]'
----
.Example output.
----
configmap/rook-ceph-operator-config patched
----

Validate that the there are the two new CSI sidecar containers per `csi-rbdplugin-provisioner` *Pod*.

[source,role="execute"]
----
for l in $(oc get pods -n openshift-storage -l app=csi-rbdplugin-provisioner -o jsonpath={.items[*].spec.containers[*].name}) ; do echo $l ; done | egrep "csi-omap-generator|volume-replication"
----
.Example output.
----
csi-omap-generator
volume-replication
csi-omap-generator
volume-replication
----

NOTE: The new containers will be repeated because there are two csi-rbdplugin-provisioner pods for redundancy.

=== [Pre-release] Create Custom ODF Catalogsource

Prior to the *ODF 4.9* being released you will need to use `pre-release` images and a custom *CatalogSource*. To do this you will need access to the `pre-release` images and to create this *CatalogSource*. 

NOTE: This custom *CatalogSource* should be created on the *Hub cluster* where ACM is installed.

Save this YAML to filename `ocs-catalogsource.yaml`.

[source,yaml]
----
---
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: ocs-catalogsource
  namespace: openshift-marketplace
spec:
  displayName: OpenShift Container Storage
  image: quay.io/rhceph-dev/ocs-registry:$tag
  sourceType: grpc
----

NOTE: The *$tag* should be replaced with the correct tag for the latest build (i.e. 4.9.0-140.ci).

[source,role="execute"]
----
oc apply -f ocs-catalogsource.yaml -n openshift-storage
----
.Example output.
----
catalogsource.operators.coreos.com/ocs-catalogsource created
----

=== Install ODF Multicluster Orchestrator

This is a new controller that will be installed from OCP *OperatorHub* on the `Hub cluster`. The job of this `Multicluster Orchestrator` controller, along with the `MirrorPeer` Custom Resource, is to create a bootstrap token and exchanges this token between the managed clusters.

Navigate to *OperatorHub* on the `Hub cluster` and filter for `odf multicluster orchestrator`.

.OperatorHub filter for ODF Multicluster Orchestrator
image::ODF-multicluster-orchestrator.png[OperatorHub filter for ODF Multicluster Orchestrator]

Keep all default settings and *Install* this operator.

.ODF Multicluster Orchestrator install
image::ODF-multicluster-orchestrator-install.png[ODF Multicluster Orchestrator install] 

NOTE: The operator resources will be installed in `openshift-operators` and available to all namespaces.

Validate successful installation by having the ability to select `View Operator`. This means the installation has completed.

.ODF Multicluster Orchestrator installed
image::ODF-multicluster-orchestrator-installed.png[ODF Multicluster Orchestrator installed] 

=== Create Mirror Peer Custom Resource

*Mirror Peer* is a cluster-scoped resource to hold information about the managed clusters that will have a `peering` relationship.

Requirements:

* Must be installed on `Hub cluster` after the `ODF Multicluster Orchestrator` is installed on `Hub cluster`.
* There can only be two clusters per Mirror Peer.
* Each cluster should be uniquely identifiable by cluster name (i.e., ocp4perf1).

After selecting `View Operator` in prior step you should see the `Mirror Peer` API. Select *Create instance* and then select *YAML view*.

.Create Mirror Peer in YAML view
image::ODF-mirror-peer-yaml.png[Create Mirror Peer in YAML view]

Save this YAML to filename `mirror-peer.yaml` after replacing *<cluster1>* and *<cluster2>* with the correct names of your managed clusters in *ACM*.

[source,yaml]
----
apiVersion: multicluster.odf.openshift.io/v1alpha1
kind: MirrorPeer
metadata:
  name: mirrorpeer-<cluster1>-<cluster2>
spec:
  items:
  - clusterName: <cluster1>
    storageClusterRef:
      name: ocs-storagecluster
      namespace: openshift-storage
  - clusterName: <cluster2>
    storageClusterRef:
      name: ocs-storagecluster
      namespace: openshift-storage
----      

Now create the `Mirror Peer` resource by copying the contents of your unique `mirror-peer.yaml` file into the `YAML view` (completely replacing original content). Select *Create* at the bottom of the `YAML view` screen.

You can also create this resource using CLI. There is no need to specify a namespace to create this resource in the following command because `MirrorPeer` is a cluster-scoped resource.

[source,role="execute"]
----
oc apply -f mirror-peer.yaml
----
.Example output.
----
mirrorpeer.multicluster.odf.openshift.io/mirrorpeer-ocp4perf1-ocp4perf2 created
----

You can validate the secret (created from token) has been exchanged with this validation command

[source]
----
oc get mirrorpeer mirrorpeer-<cluster1>-<cluster2> -o jsonpath='{.status.phase}{"\n"}'
----
.Example output.
----
ExchangedSecret
----

IMPORTANT: Before executing the command replace *<cluster1>* and *<cluster2>* with your correct values.

=== Enable Mirroring on Managed clusters

Currently to enable `mirroring` the *StorageCluster* for each managed cluster will need to have the `mirroring` setting changed to _enabled_. This is a manual step using CLI and the `oc patch` command.

IMPORTANT: Make sure to run the `oc patch storagecluster` command on the *Primary managed cluster* and the *Secondary managed cluster* as well as the follow-on validation commands after the *StorageCluster* has mirroring enabled.

[source,role="execute"]
----
oc patch storagecluster $(oc get storagecluster -n openshift-storage -o=jsonpath='{.items[0].metadata.name}')  -n openshift-storage --type json --patch  '[{ "op": "replace", "path": "/spec/mirroring", "value": {"enabled": true} }]'
----
.Example output.
----
storagecluster.ocs.openshift.io/ocs-storagecluster patched
----

Validate mirroring is enabled on default *CephBlockPool*.

[source,role="execute"]
----
oc get cephblockpool -n openshift-storage -o=jsonpath='{.items[?(@.metadata.ownerReferences[*].kind=="StorageCluster")].spec.mirroring.enabled}{"\n"}'
----
.Example output.
----
true
----

Validate `rbd-mirror` *Pod* is up and running. 

[source,role="execute"]
----
oc get pods -o name -l app=rook-ceph-rbd-mirror -n openshift-storage
----
.Example output.
----
pod/rook-ceph-rbd-mirror-a-6486c7d875-56v2v
----

Validate the status of the `daemon` health.

[source,role="execute"]
----
oc get cephblockpool ocs-storagecluster-cephblockpool -n openshift-storage -o jsonpath='{.status.mirroringStatus.summary}{"\n"}'
----
.Example output.
----
{"daemon_health":"OK","health":"OK","image_health":"OK","states":{}}
----

== Create VolumeReplicationClass resource

The *VolumeReplicationClass* is used to specify the `mirroringMode` for each volume to be replicated as well as how often a volume or image is replicated (i.e., every 5 minutes) from the local cluster to the remote cluster.

NOTE: This resource must be created on the *Primary managed cluster* and the *Secondary managed cluster*.

Save this YAML to filename `rbd-volumereplicationclass.yaml`.

[source,yaml]
----
apiVersion: replication.storage.openshift.io/v1alpha1
kind: VolumeReplicationClass
metadata:
  name: odf-rbd-volumereplicationclass
spec:
  provisioner: openshift-storage.rbd.csi.ceph.com
  parameters:
    mirroringMode: snapshot
    schedulingInterval: "5m"  # <-- Must be the same as scheduling interval in the DRPolicy
    replication.storage.openshift.io/replication-secret-name: rook-csi-rbd-provisioner
    replication.storage.openshift.io/replication-secret-namespace: openshift-storage
----

[source,role="execute"]
----
oc apply -f rbd-volumereplicationclass.yaml
----
.Example output.
----
volumereplicationclass.replication.storage.openshift.io/odf-rbd-volumereplicationclass created
----

== Install ODR Cluster operator on managed clusters

Navigate to *OperatorHub* and filter for `ODR Cluster Operator`. Follow instructions to *Install* the operator into project `openshift-operators`.

NOTE: The `ODR Cluster Operator` must be installed on both the *Primary managed cluster* and *Secondary managed cluster*.

=== Configure S3 endpoints, Secrets and ConfigMaps

This configuration is used to create object buckets used for DR failover and failback metadata. These instructions are applicable for creating the necessary object bucket(s) using the *Multi-Cloud Gateway* or MCG.

The first step is to extract the MCG access and secret keys for each managed cluster. This can be done using these commands:

[source,role="execute"]
----
oc get secret noobaa-admin -n openshift-storage -o jsonpath='{.data.AWS_ACCESS_KEY_ID}{"\n"}'
----
.Example output.
----
OVNxSmtNTU5DTzRUYld5Tmhzdlk=
----

[source,role="execute"]
----
oc get secret noobaa-admin -n openshift-storage -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}{"\n"}'
----
.Example output.
----
ZGdNYmE4cGZnY3hNSkRIaFpZUElxaUNpVm4wWFFsaDJmQzRKVmdVZA==
----

IMPORTANT: The MCG access and secret key must be retrieved from the existing MCG configuration on both the *Primary managed cluster* and *Secondary managed cluster*.

The next step is to find the external S3 endpoint or route for MCG on each managed cluster. This can be done using this command:

[source,role="execute"]
----
oc get route s3 -n openshift-storage -o jsonpath --template="http://{.spec.host}{'\n'}"
----
.Example output.
----
http://s3-openshift-storage.apps.perf1.chris.ocs.ninja
----

IMPORTANT: The S3 endpoint route must be retrieved for both the *Primary managed cluster* and *Secondary managed cluster*.
